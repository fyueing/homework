#                                           MapReduce心得体会



## 1. MapReduce的由来、定义及用途



（1）由来：MapReduce最早是由Google公司研究提出的一种面向大规模数据处理的并行计算模型和方法。MapReduce的思想来源于Google的几篇论文，里面写道MapReduce的灵感来源于函数式语言中的map和reduce。

（2）定义：MapReduce是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架。

（3）用途：在Google，MapReduce用在非常广泛的应用程序中，包括“分布grep，分布排序，web连接图反转，每台机器的词矢量，web访问日志分析，反向索引构建，文档聚类，机器学习，基于统计的机器翻译...”值得注意的是，MapReduce实现以后，它被用来重新生成Google的整个索引，并取代老的ad hoc程序去更新索引。 MapReduce会生成大量的临时文件，为了提高效率，它利用Google文件系统来管理和访问这些文件。 在谷歌，超过一万个不同的项目已经采用MapReduce来实现,包括大规模的算法图形处理、文字处理、数据挖掘、机器学习、统计机器翻译以及众多其他领域。



## 2.MapReduce的工作过程

MapReduce是用来处理数据的一种方式，总的来时就是分而治之。原理是将数据分散到不同的节点运算，以缓解单台服务器的压力，从而增加运行速度。



MapReduce的主要组成部分（如图）：

![image](C:\Users\fuyue\Desktop\微信图片_20220918215957.jpg)



**MapReduce的工作流程：**



![image](C:\Users\fuyue\Desktop\流程图 - export.png)





（1）分片、格式化：输入Map阶段的数据源，必须经过分片和格式化操作。



（2）执行 MapTask: 每个 Map 任务都有一个内存缓冲区(缓冲区大小 100MB )，输入的分片( split )数据经过 Map 任务处理后的中间结果会写入内存缓冲区中。如果写人的数据达到内存缓冲的阈值( 80MB )，会启动一个线程将内存中的溢出数据写入磁盘，同时不影响 Map 中间结果继续写入缓冲区。



(3) 执行 Shuffle 过程: MapReduce 工作过程中， Map 阶段处理的数据如何传递给 Reduce 阶段，这是 MapReduce 框架中关键的一个过程，这个过程叫作 Shuffle 。Shuffle 会将 MapTask 输出的处理结果数据分发给 ReduceTask ，并在分发的过程中，对数据按 key 进行分区和排序。



(4)  执行 ReduceTask: 输入 ReduceTask 的数据流是<key, {value list}>形式，用户可以自定义 reduce()方法进行逻辑处理，最终以<key, value>的形式输出。



(5)写入文件: MapReduce 框架会自动把 ReduceTask 生成的<key, value>传入 OutputFormat 的 write 方法，实现文件的写入操作。





## 3.MapReduce的优点：

1）MapReduce 易于编程
它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。

2）良好的扩展性
当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。

3）高容错性
MapReduce设计的初衷就是使程序能够部署在廉价的机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的。

4）适合PB级以上海量数据的离线处理
可以实现上千台服务器集群并发工作，提供数据处理能力。






## 4.MapReduce的缺点：

1.难以实时计算（MapReduce处理的是存储在本地磁盘上的离线数据）。

2.不能流式计算（MapReduce设计处理的数据源是静态的）。

3.难以DAG（有向图）计算MapReduce这些并行计算大都是基于非循环的数据流模型，也就是说，一次计算过程中，不同计算节点之间保持高度并行，这样的数据流模型使得那些需要反复使用一个特定数据集的迭代算法无法高效地运行。

